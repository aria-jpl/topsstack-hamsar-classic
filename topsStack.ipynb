{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentinel-1 TOPS stack processor\n",
    "The detailed algorithm for stack processing of TOPS data can be find here:\n",
    "\n",
    "+ Fattahi, H., P. Agram, and M. Simons (2016), A Network-Based Enhanced Spectral Diversity Approach for TOPS Time-Series Analysis, IEEE Transactions on Geoscience and Remote Sensing, 55(2), 777-786, doi:[10.1109/TGRS.2016.2614925](https://ieeexplore.ieee.org/abstract/document/7637021).\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "The scripts provides support for Sentinel-1 TOPS stack processing. Currently supported workflows include a coregistered stack of SLC, interferograms, offsets, and coherence. \n",
    "\n",
    "#### Be sure [default] credentials in ~/.aws/credentials are valid and the ~/.netrc file has been changed to include valid credentials for urs.earthdata.nasa.gov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from math import floor, ceil\n",
    "import json\n",
    "import re\n",
    "import osaka\n",
    "import osaka.main\n",
    "from builtins import str\n",
    "import os, sys, re, json, logging, traceback, requests, argparse\n",
    "from datetime import datetime\n",
    "from pprint import pformat\n",
    "from requests.packages.urllib3.exceptions import (InsecureRequestWarning,\n",
    "                                                  InsecurePlatformWarning)\n",
    "try: from html.parser import HTMLParser\n",
    "except: from html.parser import HTMLParser\n",
    "        \n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "requests.packages.urllib3.disable_warnings(InsecurePlatformWarning)\n",
    "PROCESSING_START=datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "print(PROCESSING_START)\n",
    "\n",
    "PGE_BASE=os.getcwd()\n",
    "print(PGE_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLC_RE = re.compile(r'(?P<mission>S1\\w)_IW_SLC__.*?' +\n",
    "                    r'_(?P<start_year>\\d{4})(?P<start_month>\\d{2})(?P<start_day>\\d{2})' +\n",
    "                    r'T(?P<start_hour>\\d{2})(?P<start_min>\\d{2})(?P<start_sec>\\d{2})' +\n",
    "                    r'_(?P<end_year>\\d{4})(?P<end_month>\\d{2})(?P<end_day>\\d{2})' +\n",
    "                    r'T(?P<end_hour>\\d{2})(?P<end_min>\\d{2})(?P<end_sec>\\d{2})_.*$')\n",
    "ISCE_HOME=\"/opt/isce2/isce\"\n",
    "\n",
    "QC_SERVER = 'https://qc.sentinel1.eo.esa.int/'\n",
    "DATA_SERVER = 'http://aux.sentinel1.eo.esa.int/'\n",
    "\n",
    "ORBITMAP = [('precise','aux_poeorb', 100),\n",
    "            ('restituted','aux_resorb', 100)]\n",
    "\n",
    "OPER_RE = re.compile(r'S1\\w_OPER_AUX_(?P<type>\\w+)_OPOD_(?P<yr>\\d{4})(?P<mo>\\d{2})(?P<dy>\\d{2})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "ISCE_HOME=\"/opt/isce2/isce\"\n",
    "min_lat = 34.6002832\n",
    "max_lat = 34.6502392\n",
    "min_lon = -79.0801608\n",
    "max_lon = -78.9705888\n",
    "master_date = \"\"\n",
    "localize_slcs: List[str] = [\"S1A_IW_SLC__1SDV_20150315T231319_20150315T231349_005049_006569_0664\",\n",
    "                      \"S1A_IW_SLC__1SDV_20150818T231326_20150818T231356_007324_00A0E0_93D5\",\n",
    "                      \"S1A_IW_SLC__1SDV_20150830T231332_20150830T231402_007499_00A5A9_02B3\",\n",
    "                      \"S1A_IW_SLC__1SDV_20160414T231321_20160414T231350_010824_01030B_F02B\",\n",
    "                      \"S1A_IW_SLC__1SDV_20160414T231348_20160414T231416_010824_01030B_9FA9\"\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = {}\n",
    "ctx[\"min_lat\"] = min_lat\n",
    "ctx[\"max_lat\"] = max_lat\n",
    "ctx[\"min_lon\"] = min_lon\n",
    "ctx[\"max_lon\"] = max_lon\n",
    "ctx[\"master_date\"]=\"\"\n",
    "ctx[\"localize_slcs\"]= localize_slcs\n",
    "\n",
    "print(json.dumps(ctx, indent=4))\n",
    "wd = os.getcwd()\n",
    "global runtime_dict\n",
    "runtime_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('_stdout.txt', 'w') as f:\n",
    "    f.write(\"Output File\")\n",
    "with open('_context.json', 'w') as outfile:\n",
    "    json.dump(ctx, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHTMLParser(HTMLParser):\n",
    "\n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "        self.fileList = []\n",
    "        self.pages = 0\n",
    "        self.in_td = False\n",
    "        self.in_a = False\n",
    "        self.in_ul = False\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'td':\n",
    "            self.in_td = True\n",
    "        elif tag == 'a' and self.in_td:\n",
    "            self.in_a = True\n",
    "        elif tag == 'ul':\n",
    "            for k,v in attrs:\n",
    "                if k == 'class' and v.startswith('pagination'):\n",
    "                    self.in_ul = True\n",
    "        elif tag == 'li' and self.in_ul:\n",
    "            self.pages += 1\n",
    "\n",
    "    def handle_data(self,data):\n",
    "        if self.in_td and self.in_a:\n",
    "            if OPER_RE.search(data):\n",
    "                self.fileList.append(data.strip())\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'td':\n",
    "            self.in_td = False\n",
    "            self.in_a = False\n",
    "        elif tag == 'a' and self.in_td:\n",
    "            self.in_a = False\n",
    "        elif tag == 'ul' and self.in_ul:\n",
    "            self.in_ul = False\n",
    "        elif tag == 'html':\n",
    "            if self.pages == 0:\n",
    "                self.pages = 1\n",
    "            else:\n",
    "                # decrement page back and page forward list items\n",
    "                self.pages -= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_get(session, url):\n",
    "    return session.get(url, verify=False)\n",
    "\n",
    "def get_download_orbit_dict(download_orbit_dict, slc_date, mission_type):\n",
    "    \n",
    "\n",
    "    url = \"https://qc.sentinel1.eo.esa.int/aux_poeorb/?validity_start={}&sentinel1__mission={}\".format(slc_date, mission_type)\n",
    "    session = requests.Session()\n",
    "    r = session_get(session, url)\n",
    "    r.raise_for_status()\n",
    "    parser = MyHTMLParser()\n",
    "    parser.feed(r.text)\n",
    "\n",
    "    for res in parser.fileList:\n",
    "        #id = \"%s-%s\" % (os.path.splitext(res)[0], dataset_version)\n",
    "        match = OPER_RE.search(res)\n",
    "        if not match:\n",
    "            raise RuntimeError(\"Failed to parse orbit: {}\".format(res))\n",
    "        download_orbit_dict[res] = os.path.join(DATA_SERVER, \"/\".join(match.groups()), \"{}.EOF\".format(res))\n",
    "        #yield id, results[id]\n",
    "        \n",
    "    #print(results)\n",
    "    \n",
    "    return download_orbit_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orbit_files():\n",
    "    from datetime import datetime\n",
    "    import json\n",
    "    import os\n",
    "    import osaka\n",
    "    import osaka.main\n",
    "    \n",
    "    orbit_dict = {}\n",
    "\n",
    "    orbit_dates = []\n",
    "    \n",
    "    for slc in ctx[\"localize_slcs\"]:\n",
    "        match = SLC_RE.search(slc)\n",
    "        if not match:\n",
    "            raise RuntimeError(\"Failed to recognize SLC ID %s.\" %slc)\n",
    "        mission = match.group('mission')\n",
    "        day_dt_str = \"{}-{}-{}\".format(match.group('start_year'), \n",
    "                                       match.group('start_month'), match.group('start_day'))\n",
    "        \n",
    "        if day_dt_str not in orbit_dates:\n",
    "            orbit_dates.append(day_dt_str)\n",
    "            orbit_dict = get_download_orbit_dict(orbit_dict, day_dt_str, mission)\n",
    "            directory = os.path.join(wd, \"orbits\")\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            #directory = os.path.join(wd, \"orbits\")\n",
    "            for k, v in orbit_dict.items():\n",
    "                osaka.main.get(v, directory)\n",
    "         \n",
    "    print(\"orbit_dict : %s \" %json.dumps(orbit_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_time():\n",
    "    import datetime\n",
    "    return datetime.datetime.now()\n",
    "\n",
    "def download_slc(slc_id, path):\n",
    "    url = \"https://datapool.asf.alaska.edu/SLC/SA/{}.zip\".format(slc_id)\n",
    "    print(\"Downloading {} : {}\".format(slc_id, url))\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    osaka.main.get(url, path)\n",
    "\n",
    "def run_cmd_output(cmd):\n",
    "    from subprocess import check_output, CalledProcessError\n",
    "    cmd_line = \" \".join(cmd)\n",
    "    print(\"Calling: {}\".format(cmd_line))\n",
    "    output = check_output(cmd_line, shell=True)\n",
    "    return output\n",
    "\n",
    "def run_cmd(cmd):\n",
    "    !source /opt/isce2/isce_env.sh\n",
    "    !export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:${PATH}\n",
    "    !export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:${PYTHONPATH}\n",
    "\n",
    "    import subprocess\n",
    "    from subprocess import check_call, CalledProcessError\n",
    "    import sys\n",
    "    cmd_line = \" \".join(cmd)\n",
    "    print(\"Calling : {}\".format(cmd_line))\n",
    "    p = subprocess.Popen(cmd_line, shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "    while True: \n",
    "        line = p.stdout.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        print(line.strip())\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "def get_minimum_bounding_rectangle():\n",
    "    from math import floor, ceil\n",
    "    cwd = os.getcwd()\n",
    "    slc_ids = [x for x in os.listdir('.') if os.path.isdir(x) and '_SLC__' in x]\n",
    "\n",
    "    all_lats = []\n",
    "    all_lons = []\n",
    "    for slc in slc_ids:\n",
    "        slc_met_json = slc + '.met.json'\n",
    "\n",
    "        with open(os.path.join(cwd, slc, slc_met_json), 'r') as f:\n",
    "            data = json.load(f)\n",
    "            bbox = data['bbox']\n",
    "            for coord in bbox:\n",
    "                all_lats.append(coord[0])\n",
    "                all_lons.append(coord[1])\n",
    "\n",
    "    min_lat = min(all_lats) + 0.2\n",
    "    max_lat = max(all_lats) - 0.1\n",
    "    min_lon = min(all_lons)\n",
    "    max_lon = max(all_lons)\n",
    "\n",
    "    min_lat_lo = floor(min_lat)\n",
    "    max_lat_hi = ceil(max_lat)\n",
    "    min_lon_lo = floor(min_lon)\n",
    "    max_lon_hi = ceil(max_lon)\n",
    "\n",
    "    return min_lat, max_lat, min_lon, max_lon, min_lat_lo, max_lat_hi, min_lon_lo, max_lon_hi\n",
    "\n",
    "def get_user_input_bbox(ctx_file):\n",
    "    \"\"\"\n",
    "    :param ctx_file: dictionary from cxt file\n",
    "    :return: void\n",
    "    \"\"\"\n",
    "    from math import floor, ceil\n",
    "    min_lat = ctx_file['min_lat']\n",
    "    max_lat = ctx_file['max_lat']\n",
    "    min_lon = ctx_file['min_lon']\n",
    "    max_lon = ctx_file['max_lon']\n",
    "\n",
    "    min_lat_lo = floor(min_lat)\n",
    "    max_lat_hi = ceil(max_lat)\n",
    "    min_lon_lo = floor(min_lon)\n",
    "    max_lon_hi = ceil(max_lon)\n",
    "\n",
    "    return min_lat, max_lat, min_lon, max_lon, min_lat_lo, max_lat_hi, min_lon_lo, max_lon_hi\n",
    "\n",
    "def get_master_date(ctx):\n",
    "    master_date = ctx.get('master_date', \"\")\n",
    "    return master_date\n",
    "\n",
    "def get_bbox(ctx):\n",
    "    # min_lat, max_lat, min_lon, max_lon = ctx['region_of_interest']\n",
    "\n",
    "    if ctx['min_lat'] != \"\" or ctx['max_lat'] != \"\" or ctx['min_lon'] != \"\" or ctx['max_lon'] != \"\":\n",
    "        # if any values are present in _context.json we can assume user put them in manually\n",
    "        bbox_data = get_user_input_bbox(ctx)\n",
    "    else:\n",
    "        # if user did not define ANY lat lons\n",
    "        bbox_data = get_minimum_bounding_rectangle()\n",
    "\n",
    "    return bbox_data\n",
    "\n",
    "def download_dem():\n",
    "    dem_cmd = [\n",
    "        \"{}/applications/dem.py\".format(ISCE_HOME), \"-a\",\n",
    "        \"stitch\", \"-b\", \"{} {} {} {}\".format(MINLAT_LO, MAXLAT_HI, MINLON_LO, MAXLON_HI),\n",
    "        \"-r\", \"-s\", \"1\", \"-f\", \"-c\", \"|\", \"tee\", \"dem.txt\"\n",
    "        #\"-n\", dem_user, \"-w\", dem_pass,\"-u\", dem_url\n",
    "    ]\n",
    "    run_cmd(dem_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import json\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from osgeo import ogr, osr\n",
    "\n",
    "def get_union_polygon_from_bbox(env):\n",
    "    coords = [\n",
    "        [ env[3], env[0] ],\n",
    "        [ env[3], env[1] ],\n",
    "        [ env[2], env[1] ],\n",
    "        [ env[2], env[0] ],\n",
    "    ]\n",
    "    return {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [ coords ]\n",
    "    }\n",
    "\n",
    "# copied from stitch_ifgs.get_union_polygon()\n",
    "def get_union_polygon(ds_files):\n",
    "    \"\"\"\n",
    "    Get GeoJSON polygon of union of IFGs.\n",
    "    :param ds_files: list of .dataset.json files, which have the 'location' key\n",
    "    :return: geojson of merged bbox\n",
    "    \"\"\"\n",
    "\n",
    "    geom_union = None\n",
    "    for ds_file in ds_files:\n",
    "        f = open(ds_file)\n",
    "        ds = json.load(f)\n",
    "        geom = ogr.CreateGeometryFromJson(json.dumps(ds['location'], indent=2, sort_keys=True))\n",
    "        if geom_union is None:\n",
    "            geom_union = geom\n",
    "        else:\n",
    "            geom_union = geom_union.Union(geom)\n",
    "    return json.loads(geom_union.ExportToJson()), geom_union.GetEnvelope()\n",
    "\n",
    "\n",
    "def get_dataset_met_json_files(cxt):\n",
    "    \"\"\"\n",
    "    returns 2 lists: file paths for dataset.json files and met.json files\n",
    "    :param cxt: json from _context.json\n",
    "    :return: list[str], list[str]\n",
    "    \"\"\"\n",
    "    pwd = os.getcwd()\n",
    "    localize_urls = cxt['localize_slcs']\n",
    "\n",
    "    met_files, ds_files = [], []\n",
    "    for localize_url in localize_urls:\n",
    "        local_path = localize_url['local_path']\n",
    "        slc_id = local_path.split('/')[0]\n",
    "        slc_path = os.path.join(pwd, slc_id, slc_id)\n",
    "\n",
    "        ds_files.append(slc_path + '.dataset.json')\n",
    "        met_files.append(slc_path + '.met.json')\n",
    "    return ds_files, met_files\n",
    "\n",
    "\n",
    "def get_scenes(cxt):\n",
    "    \"\"\"\n",
    "    gets all SLC scenes for the stack\n",
    "    :param cxt: contents for _context.json\n",
    "    :return: list of scenes\n",
    "    \"\"\"\n",
    "    localize_urls = cxt['localize_slcs']\n",
    "    all_scenes = set()\n",
    "    for localize_url in localize_urls:\n",
    "        local_path = localize_url['local_path']\n",
    "        slc_id = local_path.split('/')[0]\n",
    "        all_scenes.add(slc_id)\n",
    "    return sorted(list(all_scenes))\n",
    "\n",
    "\n",
    "def get_min_max_timestamps(scenes_ls):\n",
    "    \"\"\"\n",
    "    returns the min timestamp and max timestamp of the stack\n",
    "    :param scenes_ls: list[str] all slc scenes in stack\n",
    "    :return: (str, str) 2 timestamp strings, ex. 20190518T161611\n",
    "    \"\"\"\n",
    "    timestamps = set()\n",
    "\n",
    "    regex_pattern = r'(\\d{8}T\\d{6}).(\\d{8}T\\d{6})'\n",
    "    for scene in scenes_ls:\n",
    "        matches = re.search(regex_pattern, scene)\n",
    "        if not matches:\n",
    "            raise Exception(\"regex %s was unable to match with SLC id %s\" % (regex_pattern, scene))\n",
    "\n",
    "        slc_timestamps = (matches.group(1), matches.group(2))\n",
    "        timestamps = timestamps.union(slc_timestamps)\n",
    "\n",
    "    min_timestamp = min(timestamps)\n",
    "    max_timestamp = max(timestamps)\n",
    "    return min_timestamp.replace('T', ''), max_timestamp.replace('T', '')\n",
    "\n",
    "def get_min_max_times(scenes_ls):\n",
    "    \"\"\"\n",
    "    returns the min timestamp and max timestamp of the stack\n",
    "    :param scenes_ls: list[str] all slc scenes in stack\n",
    "    :return: (str, str) 2 timestamp strings, ex. 20190518T161611\n",
    "    \"\"\"\n",
    "    timestamps = set()\n",
    "\n",
    "    regex_pattern = r'(\\d{8}T\\d{6}).(\\d{8}T\\d{6})'\n",
    "    for scene in scenes_ls:\n",
    "        matches = re.search(regex_pattern, scene)\n",
    "        if not matches:\n",
    "            raise Exception(\"regex %s was unable to match with SLC id %s\" % (regex_pattern, scene))\n",
    "\n",
    "        slc_timestamps = (matches.group(1), matches.group(2))\n",
    "        timestamps = timestamps.union(slc_timestamps)\n",
    "\n",
    "    min_timestamp = min(timestamps)\n",
    "    max_timestamp = max(timestamps)\n",
    "    return min_timestamp, max_timestamp\n",
    "\n",
    "\n",
    "def create_list_from_keys_json_file(json_files, *args):\n",
    "    \"\"\"\n",
    "    gets all key values in each .json file and returns a sorted array of values\n",
    "    :param json_files: list[str]\n",
    "    :return: list[]\n",
    "    \"\"\"\n",
    "    values = set()\n",
    "    for json_file in json_files:\n",
    "        if os.path.isfile(json_file): \n",
    "            f = open(json_file)\n",
    "            data = json.load(f)\n",
    "            for arg in args:\n",
    "                value = data[arg]\n",
    "                values.add(value)\n",
    "    return sorted(list(values))\n",
    "\n",
    "\n",
    "def camelcase_to_underscore(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "\n",
    "def get_key_and_convert_to_underscore(json_file_paths, key):\n",
    "    \"\"\"\n",
    "    read through all the json files in file paths, get the first occurrence of key and convert it to underscore\n",
    "    :param json_file_paths: list[str]\n",
    "    :param key: str\n",
    "    :return: key and value\n",
    "    \"\"\"\n",
    "    for json_file in json_file_paths:\n",
    "        if os.path.isfile(json_file): \n",
    "            f = open(json_file)\n",
    "            data = json.load(f)\n",
    "            if key in data.keys():\n",
    "                underscore_key = camelcase_to_underscore(key)\n",
    "                return underscore_key, data[key]\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def generate_dataset_json_data(ctx, version):\n",
    "    \"\"\"\n",
    "    :param cxt: _context.json file\n",
    "    :param dataset_json_files: list[str] all file paths of SLC's .dataset.json files\n",
    "    :param version: str: version, ex. v1.0\n",
    "    :return: dict\n",
    "    \"\"\"\n",
    "    dataset_json_data = dict()\n",
    "    dataset_json_data['version'] = version\n",
    "\n",
    "    \n",
    "    try:      \n",
    "        dataset_json_data['starttime'], dataset_json_data['endtime']  = get_min_max_times(ctx[\"localize_slcs\"])      \n",
    "    except Exception as err:\n",
    "        print(str(err))\n",
    "    try:\n",
    "        dataset_json_data['location'] = get_union_polygon_from_bbox(bbox.split(' '))\n",
    "    except Exception as err:\n",
    "        print(str(err))\n",
    "    return dataset_json_data\n",
    "\n",
    "\n",
    "def generate_met_json_data(ctx, bbox, version):\n",
    "    \"\"\"\n",
    "    :param cxt: _context.json file\n",
    "    :param met_json_file_paths: list[str] all file paths of SLC's .met.json files\n",
    "    :param dataset_json_files: list[str] all file paths of SLC's .dataset.json files\n",
    "    :param version: str: version, ex. v1.0\n",
    "    :return: dict\n",
    "    \"\"\"\n",
    "    met_json_data = {\n",
    "        'processing_start': PROCESSING_START,\n",
    "        'processing_stop': datetime.now().strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "        'version': version\n",
    "    }\n",
    "\n",
    "    # generating bbox\n",
    "    geojson = get_union_polygon_from_bbox(bbox)\n",
    "    coordinates = geojson['coordinates'][0]\n",
    "    for coordinate in coordinates:\n",
    "        coordinate[0], coordinate[1] = coordinate[1], coordinate[0]\n",
    "    \n",
    "    met_json_data['bbox'] = bbox\n",
    "    \n",
    "    # list of SLC scenes\n",
    "   \n",
    "    met_json_data['scenes'] = ctx[\"localize_slcs\"]\n",
    "    met_json_data['scene_count'] = len(ctx[\"localize_slcs\"])\n",
    "\n",
    "    # getting timestamps\n",
    "\n",
    "    met_json_data['sensing_start'], met_json_data['sensing_stop']  = get_min_max_times(ctx[\"localize_slcs\"])\n",
    "\n",
    "    # additional information\n",
    "    met_json_data['dataset_type'] = 'stack'\n",
    "\n",
    "    return met_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_context():\n",
    "    with open('_context.json', 'r') as f:\n",
    "        cxt = json.load(f)\n",
    "        return cxt\n",
    "\n",
    "\n",
    "def create_dataset(bbox):\n",
    "    VERSION = 'v1.0'\n",
    "    DATASET_NAMING_TEMPLATE = 'coregistered_slcs-{min_timestamp}-{max_timestamp}'\n",
    "    PWD = os.getcwd()\n",
    "\n",
    "    # creating list of all SLC .dataset.json and .met.json files\n",
    "    ctx = read_context()\n",
    "    #dataset_json_files, met_json_files = get_dataset_met_json_files(context_json)\n",
    "\n",
    "    # getting list of SLC scenes and extracting min max timestamp\n",
    "    slc_scenes = ctx['localize_slcs']\n",
    "    min_timestamp, max_timestamp = get_min_max_timestamps(slc_scenes)\n",
    "\n",
    "    # creatin dataset directory\n",
    "    dataset_name = DATASET_NAMING_TEMPLATE.format(min_timestamp=min_timestamp, max_timestamp=max_timestamp)\n",
    "    if not os.path.exists(dataset_name):\n",
    "        os.mkdir(dataset_name)\n",
    "\n",
    "    # move merged/ master/ slaves/ directory to dataset directory\n",
    "    move_directories = ['merged', 'reference', 'secondarys']\n",
    "    for directory in move_directories:\n",
    "        shutil.move(directory, dataset_name)\n",
    "\n",
    "    # move _stdout.txt log file to dataset\n",
    "    shutil.copyfile('_stdout.txt', os.path.join(dataset_name, '_stdout.txt'))\n",
    "\n",
    "    # generate .dataset.json data\n",
    "    dataset_json_data = {}\n",
    "    try:\n",
    "        dataset_json_data = generate_dataset_json_data(ctx, VERSION)\n",
    "    except Exception as err:\n",
    "        print(str(err))\n",
    "    dataset_json_data['label'] = dataset_name\n",
    "    print(json.dumps(dataset_json_data, indent=2))\n",
    "\n",
    "    # generate .met.json data\n",
    "    met_json_data = generate_met_json_data(ctx, bbox, VERSION)\n",
    "    print(json.dumps(met_json_data, indent=2))\n",
    "\n",
    "    # writing .dataset.json to file\n",
    "    dataset_json_filename = os.path.join(PWD, dataset_name, dataset_name + '.dataset.json')\n",
    "    with open(dataset_json_filename, 'w') as f:\n",
    "        json.dump(dataset_json_data, f, indent=2)\n",
    "\n",
    "    # writing .met.json to file\n",
    "    met_json_filename = os.path.join(PWD, dataset_name, dataset_name + '.met.json')\n",
    "    with open(met_json_filename, 'w') as f:\n",
    "        json.dump(met_json_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "MINLAT, MAXLAT, MINLON, MAXLON, MINLAT_LO, MAXLAT_HI, MINLON_LO, MAXLON_HI =get_bbox(ctx)\n",
    "print(\"{} {} {} {} {} {} {} {}\".format(MINLAT, MAXLAT, MINLON, MAXLON, MINLAT_LO, MAXLAT_HI, MINLON_LO, MAXLON_HI))\n",
    "\n",
    "bbox = \"{} {} {} {}\".format(MINLAT, MAXLAT, MINLON, MAXLON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Download Sentinel-1 data SLC ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_slcs(path):\n",
    "    \n",
    "    for slc in ctx[\"localize_slcs\"]:\n",
    "        download_slc(slc, path)\n",
    "        \n",
    "path = \"zip\"\n",
    "download_slcs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Download Orbit Files based onn SLC ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_orbit_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(\"cwd : {}\".format(cwd))\n",
    "if os.path.exists(\"dem.txt\"):\n",
    "    cmd = [\"awk\", \"'/wgs84/ {print $NF;exit}'\", \"dem.txt\"]\n",
    "    WGS84 = run_cmd_output(cmd).decode(\"utf-8\").strip()\n",
    "    wgs84_file = os.path.join(cwd, WGS84)\n",
    "    print(\"WGS84 : a{}b\".format(wgs84_file))\n",
    "    if os.path.exists(wgs84_file):\n",
    "        print(\"Found wgs84 file: {}\".format(wgs84_file))\n",
    "        fix_cmd = [\"{}/applications/fixImageXml.py\".format(ISCE_HOME), \"--full\", \"-i\", \"{}\".format(wgs84_file) ]\n",
    "        run_cmd(fix_cmd) \n",
    "    else:\n",
    "        print(\"NO WGS84 FILE FOUND : {}\".format(wgs84_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUX_CAL file download ####\n",
    "\n",
    "The following calibration auxliary (AUX_CAL) file is used for **antenna pattern correction** to compensate the range phase offset of SAFE products with **IPF verison 002.36** (mainly for images acquired before March 2015). If all your SAFE products are from another IPF version, then no AUX files are needed. Check [ESA document](https://earth.esa.int/documents/247904/1653440/Sentinel-1-IPF_EAP_Phase_correction) for details. \n",
    "\n",
    "Run the command below to download the AUX_CAL file once and store it somewhere (_i.e._ ~/aux/aux_cal) so that you can use it all the time, for `stackSentinel.py -a` or `auxiliary data directory` in `topsApp.py`.\n",
    "\n",
    "```\n",
    "wget https://qc.sentinel1.eo.esa.int/product/S1A/AUX_CAL/20140908T000000/S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ\n",
    "tar zxvf S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ\n",
    "rm S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ./AuxDir\n",
    "wget https://qc.sentinel1.eo.esa.int/product/S1A/AUX_CAL/20140908T000000/S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ\n",
    "tar zxvf S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ --directory ./AuxDir\n",
    "rm S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The scripts provides support for Sentinel-1 TOPS stack processing. Currently supported workflows include a coregistered stack of SLC, interferograms, offsets, and coherence. \n",
    "\n",
    "`stackSentinel.py` generates all configuration and run files required to be executed on a stack of Sentinel-1 TOPS data. When stackSentinel.py is executed for a given workflow (-W option) a **configs** and **run_files** folder is generated. No processing is performed at this stage. Within the run_files folder different run\\_#\\_description files are contained which are to be executed as shell scripts in the run number order. Each of these run scripts call specific configure files contained in the “configs” folder which call ISCE in a modular fashion. The configure and run files will change depending on the selected workflow. To make run_# files executable, change the file permission accordingly (e.g., `chmod +x run_01_unpack_slc`).\n",
    "\n",
    "```bash\n",
    "stackSentinel.py -H     #To see workflow examples,\n",
    "stackSentinel.py -h     #To get an overview of all the configurable parameters\n",
    "```\n",
    "\n",
    "Required parameters of stackSentinel.py include:\n",
    "\n",
    "```cfg\n",
    "-s SLC_DIRNAME          #A folder with downloaded Sentinel-1 SLC’s. \n",
    "-o ORBIT_DIRNAME        #A folder containing the Sentinel-1 orbits. Missing orbit files will be downloaded automatically\n",
    "-a AUX_DIRNAME          #A folder containing the Sentinel-1 Auxiliary files\n",
    "-d DEM_FILENAME         #A DEM (Digital Elevation Model) referenced to wgs84\n",
    "```\n",
    "\n",
    "In the following, different workflow examples are provided. Note that stackSentinel.py only generates the run and configure files. To perform the actual processing, the user will need to execute each run file in their numbered order.\n",
    "\n",
    "In all workflows, coregistration (-C option) can be done using only geometry (set option = geometry) or with geometry plus refined azimuth offsets through NESD (set option = NESD) approach, the latter being the default. For the NESD coregistrstion the user can control the ESD coherence threshold (-e option) and the number of overlap interferograms (-O) to be used in NESD estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_date=get_master_date(ctx)\n",
    "print(\"master_date : {}\".format(master_date))\n",
    "\n",
    "if master_date:\n",
    "    print(\"MASTER_DATE exists:\".format(master_date) )   \n",
    "    cmd = [\n",
    "        \"/opt/isce2/src/isce2/contrib/stack/topsStack/stackSentinel.py\",  \"-s\", \"zip/\", \"-d\", \"{}\".format(wgs84_file), \"-a\", \"AuxDir/\", \"-m\", \"{}\".format(master_date), \"-o\", \"orbits\", \n",
    "        \"-b\", \"\\\"{} {} {} {}\\\"\".format(MINLAT, MAXLAT, MINLON, MAXLON), \n",
    "        \"-W\", \"slc\", \"-C\", \"geometry\"\n",
    "    ]\n",
    "else:\n",
    "    print(\"MASTER_DATE DOES NOT EXIST\")          \n",
    "    cmd = [\n",
    "        \"/opt/isce2/src/isce2/contrib/stack/topsStack/stackSentinel.py\",  \"-s\", \"zip/\", \"-d\", \"{}\".format(wgs84_file), \"-a\", \"AuxDir/\", \"-o\", \"orbits\", \n",
    "        \"-b\", \"\\\"{} {} {} {}\\\"\".format(MINLAT, MAXLAT, MINLON, MAXLON), \n",
    "        \"-W\", \"slc\", \"-C\", \"geometry\"\n",
    "    ]\n",
    "run_cmd(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\"~/topsstack-hamsar/topsStack/stackSlcDn_run2.5.sh\", \"{} {} {} {}\".format(MINLAT, MAXLAT, MINLON, MAXLON)]\n",
    "run_cmd(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**run_01_unpack_slc_topo_reference:**\n",
    "\n",
    "Includes commands to unpack Sentinel-1 TOPS SLCs using ISCE readers. For older SLCs which need antenna elevation pattern correction, the file is extracted and written to disk. For newer version of SLCs which don’t need the elevation antenna pattern correction, only a gdal virtual “vrt” file (and isce xml file) is generated. The “.vrt” file points to the Sentinel SLC file and reads them whenever required during the processing. If a user wants to write the “.vrt” SLC file to disk, it can be done easily using gdal_translate (e.g. gdal_translate –of ENVI File.vrt File.slc). \n",
    "The “run_01_unpack_slc_topo_reference” also includes a command that refers to the config file of the stack reference, which includes configuration for running topo for the stack reference. Note that in the pair-wise processing strategy one should run topo (mapping from range-Doppler to geo coordinate) for all pairs. However, with stackSentinel, topo needs to be run only one time for the reference in the stack. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict[\"s1_start\"]=get_current_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"## STEP 1 ##\"\n",
    "\n",
    "start=`date +%s`\n",
    "echo \"sh run_files/run_01_unpack_topo_reference\"\n",
    "sh run_files/run_01_unpack_topo_reference\n",
    "end=`date +%s`\n",
    "runtime1=$((end-start))\n",
    "echo $runtime1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict[\"s1_stop\"]=get_current_time()\n",
    "runtime_dict[\"s1_runtime\"]=runtime_dict[\"s1_stop\"]-runtime_dict[\"s1_start\"]\n",
    "print(runtime_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**run_02_unpack_secondary_slc:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"## STEP 2 ##\"\n",
    "\n",
    "start=`date +%s`\n",
    "Num=`cat run_files/run_02_unpack_secondary_slc | wc | awk '{print $1}'`\n",
    "echo $Num\n",
    "echo \"cat run_files/run_02_unpack_secondary_slc | parallel -j2 --eta --load 50%\"\n",
    "cat run_files/run_02_unpack_secondary_slc | parallel -j2 --eta --load 50%\n",
    "end=`date +%s`\n",
    "\n",
    "runtime2=$((end-start))\n",
    "echo runtime2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict[\"s2_stop\"]=get_current_time()\n",
    "runtime_dict[\"s2_runtime\"]=runtime_dict[\"s2_stop\"]-runtime_dict[\"s1_stop\"]\n",
    "print(runtime_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run_02.5_slc_noise_calibration ###\n",
    "STEP 2.5 run radiometric and thermal noise calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"## STEP 2.5 ##\"\n",
    "\n",
    "start=`date +%s`\n",
    "echo \"cat run_files/run_02.5_slc_noise_calibration | parallel -j2 --eta --load 50%\"\n",
    "cat run_files/run_02.5_slc_noise_calibration | parallel -j2 --eta --load 50%\n",
    "end=`date +%s`\n",
    "\n",
    "runtime2x5=$((end-start))\n",
    "echo $runtime2x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict[\"s2.5_stop\"]=get_current_time()\n",
    "runtime_dict[\"s2.5_runtime\"]=runtime_dict[\"s2.5_stop\"]-runtime_dict[\"s2_stop\"]\n",
    "print(runtime_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**run_03_average_baseline:**\n",
    "\n",
    "Computes average baseline for the stack. These baselines are not used for processing anywhere. They are only an approximation and can be used for plotting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"## STEP 3 ##\"\n",
    "\n",
    "start=`date +%s`\n",
    "echo \"cat run_files/run_03_average_baseline | parallel -j2 --eta --load 50%\"\n",
    "cat run_files/run_03_average_baseline | parallel -j2 --eta --load 50%\n",
    "end=`date +%s`\n",
    "runtime3=$((end-start))\n",
    "echo $runtime3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict[\"s3_stop\"]=get_current_time()\n",
    "runtime_dict[\"s3_runtime\"]=runtime_dict[\"s3_stop\"]-runtime_dict[\"s2.5_stop\"]\n",
    "print(runtime_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 4 : run_04_fullBurst_geo2rdr:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"## STEP 4 ##\"\n",
    "\n",
    "start=`date +%s`\n",
    "\n",
    "echo \"cat run_files/run_04_fullBurst_geo2rdr  | parallel -j2 --eta --load 50%\"\n",
    "cat run_files/run_04_fullBurst_geo2rdr  | parallel -j2 --eta --load 50%\n",
    "end=`date +%s`\n",
    "runtime4=$((end-start))\n",
    "echo $runtime4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict[\"s4_stop\"]=get_current_time()\n",
    "runtime_dict[\"s4_runtime\"]=runtime_dict[\"s4_stop\"]-runtime_dict[\"s3_stop\"]\n",
    "print(runtime_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 5 : run_05_fullBurst_resample:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"## STEP 5 ##\"\n",
    "\n",
    "start=`date +%s`\n",
    "echo \"cat run_files/run_05_fullBurst_resample  | parallel -j2 --eta --load 50%\"\n",
    "cat run_files/run_05_fullBurst_resample  | parallel -j2 --eta --load 50%\n",
    "end=`date +%s`\n",
    "runtime5=$((end-start))\n",
    "echo $runtime5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict[\"s5_stop\"]=get_current_time()\n",
    "runtime_dict[\"s5_runtime\"]=runtime_dict[\"s5_stop\"]-runtime_dict[\"s4_stop\"]\n",
    "print(runtime_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 6 : run_06_timeseries_misreg:**\n",
    "\n",
    "A time-series of azimuth and range misregistration is estimated with respect to the stack reference. The time-series is a least squares esatimation from the pair misregistration from the previous step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"## STEP 6 ##\"\n",
    "\n",
    "start=`date +%s`\n",
    "echo \"sh run_files/run_06_extract_stack_valid_region\"\n",
    "sh run_files/run_06_extract_stack_valid_region\n",
    "end=`date +%s`\n",
    "runtime6=$((end-start))\n",
    "echo $runtime6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict[\"s6_stop\"]=get_current_time()\n",
    "runtime_dict[\"s6_runtime\"]=runtime_dict[\"s6_stop\"]-runtime_dict[\"s5_stop\"]\n",
    "print(runtime_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**run_07_geo2rdr_resample:**\n",
    "\n",
    "Using orbit and DEM, geometrical offsets among all secondary SLCs and the stack reference is computed. The geometrical offsets, together with the misregistration time-series (from previous step) are used for precise coregistration of each burst SLC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"## STEP 7 ##\"\n",
    "\n",
    "start=`date +%s`\n",
    "\n",
    "FILE=run_files/run_07_merge\n",
    "if test -f \"$FILE\"; then\n",
    "    echo \"cat run_files/run_07_merge  | parallel -j2 --eta --load 50%\"\n",
    "    cat run_files/run_07_merge | parallel -j2 --eta --load 50%\n",
    "else\n",
    "    echo \"cat run_files/run_07_merge_reference_secondary_slc  | parallel -j2 --eta --load 50%\"\n",
    "    cat run_files/run_07_merge_reference_secondary_slc | parallel -j2 --eta --load 50%\n",
    "fi\n",
    "\n",
    "end=`date +%s`\n",
    "runtime7=$((end-start))\n",
    "echo $runtime7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict[\"s7_stop\"]=get_current_time()\n",
    "runtime_dict[\"s7_runtime\"]=runtime_dict[\"s7_stop\"]-runtime_dict[\"s6_stop\"]\n",
    "print(runtime_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time before creating dataset : {}\".format(get_current_time()))\n",
    "create_dataset(bbox)\n",
    "print(\"time after creating dataset : {}\".format(get_current_time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
