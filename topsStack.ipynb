{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentinel-1 TOPS stack processor\n",
    "The detailed algorithm for stack processing of TOPS data can be find here:\n",
    "\n",
    "+ Fattahi, H., P. Agram, and M. Simons (2016), A Network-Based Enhanced Spectral Diversity Approach for TOPS Time-Series Analysis, IEEE Transactions on Geoscience and Remote Sensing, 55(2), 777-786, doi:[10.1109/TGRS.2016.2614925](https://ieeexplore.ieee.org/abstract/document/7637021).\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "The scripts provides support for Sentinel-1 TOPS stack processing. Currently supported workflows include a coregistered stack of SLC, interferograms, offsets, and coherence. \n",
    "\n",
    "#### Be sure [default]  the ~/.netrc file has been changed to include valid credentials for urs.earthdata.nasa.gov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_time():\n",
    "    import datetime\n",
    "    return datetime.datetime.now()\n",
    "run_start_time = get_current_time()\n",
    "print(\"PGE run start time : {}\".format(run_start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install git+https://github.com/hysds/osaka.git#egg=osaka\n",
    "pip install opencv-python-headless\n",
    "conda install -c conda-forge fiona -y\n",
    "conda install -c conda-forge parallel -y\n",
    "source /opt/conda/etc/profile.d/conda.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from math import floor, ceil\n",
    "import json\n",
    "import re\n",
    "import osaka\n",
    "import osaka.main\n",
    "from builtins import str\n",
    "import os, sys, re, json, logging, traceback, requests, argparse\n",
    "from datetime import datetime\n",
    "from pprint import pformat\n",
    "from requests.packages.urllib3.exceptions import (InsecureRequestWarning,\n",
    "                                                  InsecurePlatformWarning)\n",
    "try: from html.parser import HTMLParser\n",
    "except: from html.parser import HTMLParser\n",
    "        \n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "requests.packages.urllib3.disable_warnings(InsecurePlatformWarning)\n",
    "PROCESSING_START=datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "print(PROCESSING_START)\n",
    "\n",
    "PGE_BASE=os.getcwd()\n",
    "print(PGE_BASE)\n",
    "\n",
    "SLC_RE = re.compile(r'(?P<mission>S1\\w)_IW_SLC__.*?' +\n",
    "                    r'_(?P<start_year>\\d{4})(?P<start_month>\\d{2})(?P<start_day>\\d{2})' +\n",
    "                    r'T(?P<start_hour>\\d{2})(?P<start_min>\\d{2})(?P<start_sec>\\d{2})' +\n",
    "                    r'_(?P<end_year>\\d{4})(?P<end_month>\\d{2})(?P<end_day>\\d{2})' +\n",
    "                    r'T(?P<end_hour>\\d{2})(?P<end_min>\\d{2})(?P<end_sec>\\d{2})_.*$')\n",
    "ISCE_HOME=\"/opt/isce2/isce\"\n",
    "\n",
    "QC_SERVER = 'https://qc.sentinel1.eo.esa.int/'\n",
    "DATA_SERVER = 'http://aux.sentinel1.eo.esa.int/'\n",
    "\n",
    "ORBITMAP = [('precise','aux_poeorb', 100),\n",
    "            ('restituted','aux_resorb', 100)]\n",
    "\n",
    "OPER_RE = re.compile(r'S1\\w_OPER_AUX_(?P<type>\\w+)_OPOD_(?P<yr>\\d{4})(?P<mo>\\d{2})(?P<dy>\\d{2})')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "ISCE_HOME=\"/opt/isce2/isce\"\n",
    "min_lat = 34.6002832\n",
    "max_lat = 34.6502392\n",
    "min_lon = -79.0801608\n",
    "max_lon = -78.9705888\n",
    "master_date = \"\"\n",
    "localize_slcs: List[str] = [\"S1A_IW_SLC__1SDV_20150315T231319_20150315T231349_005049_006569_0664\",\n",
    "                      \"S1A_IW_SLC__1SDV_20150818T231326_20150818T231356_007324_00A0E0_93D5\",\n",
    "                      \"S1A_IW_SLC__1SDV_20150830T231332_20150830T231402_007499_00A5A9_02B3\",\n",
    "                      \"S1A_IW_SLC__1SDV_20160414T231321_20160414T231350_010824_01030B_F02B\",\n",
    "                      \"S1A_IW_SLC__1SDV_20160414T231348_20160414T231416_010824_01030B_9FA9\"\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = {}\n",
    "ctx[\"min_lat\"] = min_lat\n",
    "ctx[\"max_lat\"] = max_lat\n",
    "ctx[\"min_lon\"] = min_lon\n",
    "ctx[\"max_lon\"] = max_lon\n",
    "ctx[\"master_date\"]=\"\"\n",
    "ctx[\"localize_slcs\"]= localize_slcs\n",
    "\n",
    "print(json.dumps(ctx, indent=4))\n",
    "wd = os.getcwd()\n",
    "global runtime_dict\n",
    "runtime_dict = {}\n",
    "\n",
    "with open('_stdout.txt', 'w') as f:\n",
    "    f.write(\"Output File\")\n",
    "with open('_context.json', 'w') as outfile:\n",
    "    json.dump(ctx, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHTMLParser(HTMLParser):\n",
    "\n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "        self.fileList = []\n",
    "        self.pages = 0\n",
    "        self.in_td = False\n",
    "        self.in_a = False\n",
    "        self.in_ul = False\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'td':\n",
    "            self.in_td = True\n",
    "        elif tag == 'a' and self.in_td:\n",
    "            self.in_a = True\n",
    "        elif tag == 'ul':\n",
    "            for k,v in attrs:\n",
    "                if k == 'class' and v.startswith('pagination'):\n",
    "                    self.in_ul = True\n",
    "        elif tag == 'li' and self.in_ul:\n",
    "            self.pages += 1\n",
    "\n",
    "    def handle_data(self,data):\n",
    "        if self.in_td and self.in_a:\n",
    "            if OPER_RE.search(data):\n",
    "                self.fileList.append(data.strip())\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'td':\n",
    "            self.in_td = False\n",
    "            self.in_a = False\n",
    "        elif tag == 'a' and self.in_td:\n",
    "            self.in_a = False\n",
    "        elif tag == 'ul' and self.in_ul:\n",
    "            self.in_ul = False\n",
    "        elif tag == 'html':\n",
    "            if self.pages == 0:\n",
    "                self.pages = 1\n",
    "            else:\n",
    "                # decrement page back and page forward list items\n",
    "                self.pages -= 2\n",
    "\n",
    "def session_get(session, url):\n",
    "    return session.get(url, verify=False)\n",
    "\n",
    "def get_download_orbit_dict(download_orbit_dict, slc_date, mission_type):\n",
    "    \n",
    "\n",
    "    url = \"https://qc.sentinel1.eo.esa.int/aux_poeorb/?validity_start={}&sentinel1__mission={}\".format(slc_date, mission_type)\n",
    "    session = requests.Session()\n",
    "    r = session_get(session, url)\n",
    "    r.raise_for_status()\n",
    "    parser = MyHTMLParser()\n",
    "    parser.feed(r.text)\n",
    "\n",
    "    for res in parser.fileList:\n",
    "        #id = \"%s-%s\" % (os.path.splitext(res)[0], dataset_version)\n",
    "        match = OPER_RE.search(res)\n",
    "        if not match:\n",
    "            raise RuntimeError(\"Failed to parse orbit: {}\".format(res))\n",
    "        download_orbit_dict[res] = os.path.join(DATA_SERVER, \"/\".join(match.groups()), \"{}.EOF\".format(res))\n",
    "        #yield id, results[id]\n",
    "        \n",
    "    #print(results)\n",
    "    \n",
    "    return download_orbit_dict\n",
    "\n",
    "def get_orbit_files():\n",
    "    from datetime import datetime, timedelta\n",
    "    import json\n",
    "    import os\n",
    "    import osaka\n",
    "    import osaka.main\n",
    "    \n",
    "    orbit_dict = {}\n",
    "\n",
    "    orbit_dates = []\n",
    "    \n",
    "    for slc in ctx[\"localize_slcs\"]:\n",
    "        match = SLC_RE.search(slc)\n",
    "        if not match:\n",
    "            raise RuntimeError(\"Failed to recognize SLC ID %s.\" %slc)\n",
    "        mission = match.group('mission')\n",
    "        day_dt_str = \"{}-{}-{}\".format(match.group('start_year'), \n",
    "                                       match.group('start_month'), match.group('start_day'))\n",
    "        \n",
    "        day_dt = datetime.strptime(day_dt_str, '%Y-%m-%d') - timedelta(days=1)       \n",
    "        day_dt_str = day_dt.strftime('%Y-%m-%d')     \n",
    "        print(\"day_dt_str : {}\".format(day_dt_str))\n",
    "        if day_dt_str not in orbit_dates:\n",
    "            orbit_dates.append(day_dt_str)\n",
    "            orbit_dict = get_download_orbit_dict(orbit_dict, day_dt_str, mission)\n",
    "            directory = os.path.join(wd, \"orbits\")\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            #directory = os.path.join(wd, \"orbits\")\n",
    "            for k, v in orbit_dict.items():\n",
    "                osaka.main.get(v, directory)\n",
    "         \n",
    "    print(\"orbit_dict : %s \" %json.dumps(orbit_dict, indent=4))\n",
    "\n",
    "    \n",
    "def download_slc(slc_id, path):\n",
    "    from urllib.parse import urlparse\n",
    "    url = \"https://datapool.asf.alaska.edu/SLC/SA/{}.zip\".format(slc_id)\n",
    "    print(\"Downloading {} : {}\".format(slc_id, url))\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    if not os.path.exists(os.path.join(path, os.path.basename(urlparse(url).path) ) ):               # Output: /kyle/09-09-201315-47-571378756077.jpg\n",
    "        osaka.main.get(url, path)\n",
    "\n",
    "def run_cmd_output(cmd):\n",
    "    from subprocess import check_output, CalledProcessError\n",
    "    cmd_line = \" \".join(cmd)\n",
    "    print(\"Calling: {}\".format(cmd_line))\n",
    "    output = check_output(cmd_line, shell=True)\n",
    "    return output\n",
    "\n",
    "def run_cmd(cmd):\n",
    "    !source /opt/isce2/isce_env.sh\n",
    "    !export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:${PATH}\n",
    "    !export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:${PYTHONPATH}\n",
    "\n",
    "    import subprocess\n",
    "    from subprocess import check_call, CalledProcessError\n",
    "    import sys\n",
    "    cmd_line = \" \".join(cmd)\n",
    "    print(\"Calling : {}\".format(cmd_line))\n",
    "    p = subprocess.Popen(cmd_line, shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "    while True: \n",
    "        line = p.stdout.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        print(line.strip())\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "def get_minimum_bounding_rectangle():\n",
    "    from math import floor, ceil\n",
    "    cwd = os.getcwd()\n",
    "    slc_ids = [x for x in os.listdir('.') if os.path.isdir(x) and '_SLC__' in x]\n",
    "\n",
    "    all_lats = []\n",
    "    all_lons = []\n",
    "    for slc in slc_ids:\n",
    "        slc_met_json = slc + '.met.json'\n",
    "\n",
    "        with open(os.path.join(cwd, slc, slc_met_json), 'r') as f:\n",
    "            data = json.load(f)\n",
    "            bbox = data['bbox']\n",
    "            for coord in bbox:\n",
    "                all_lats.append(coord[0])\n",
    "                all_lons.append(coord[1])\n",
    "\n",
    "    min_lat = min(all_lats) + 0.2\n",
    "    max_lat = max(all_lats) - 0.1\n",
    "    min_lon = min(all_lons)\n",
    "    max_lon = max(all_lons)\n",
    "\n",
    "    min_lat_lo = floor(min_lat)\n",
    "    max_lat_hi = ceil(max_lat)\n",
    "    min_lon_lo = floor(min_lon)\n",
    "    max_lon_hi = ceil(max_lon)\n",
    "\n",
    "    return min_lat, max_lat, min_lon, max_lon, min_lat_lo, max_lat_hi, min_lon_lo, max_lon_hi\n",
    "\n",
    "def get_user_input_bbox(ctx_file):\n",
    "    \"\"\"\n",
    "    :param ctx_file: dictionary from cxt file\n",
    "    :return: void\n",
    "    \"\"\"\n",
    "    from math import floor, ceil\n",
    "    min_lat = ctx_file['min_lat']\n",
    "    max_lat = ctx_file['max_lat']\n",
    "    min_lon = ctx_file['min_lon']\n",
    "    max_lon = ctx_file['max_lon']\n",
    "\n",
    "    min_lat_lo = floor(min_lat)\n",
    "    max_lat_hi = ceil(max_lat)\n",
    "    min_lon_lo = floor(min_lon)\n",
    "    max_lon_hi = ceil(max_lon)\n",
    "\n",
    "    return min_lat, max_lat, min_lon, max_lon, min_lat_lo, max_lat_hi, min_lon_lo, max_lon_hi\n",
    "\n",
    "def get_master_date(ctx):\n",
    "    master_date = ctx.get('master_date', \"\")\n",
    "    return master_date\n",
    "\n",
    "def get_bbox(ctx):\n",
    "    # min_lat, max_lat, min_lon, max_lon = ctx['region_of_interest']\n",
    "\n",
    "    if ctx['min_lat'] != \"\" or ctx['max_lat'] != \"\" or ctx['min_lon'] != \"\" or ctx['max_lon'] != \"\":\n",
    "        # if any values are present in _context.json we can assume user put them in manually\n",
    "        bbox_data = get_user_input_bbox(ctx)\n",
    "    else:\n",
    "        # if user did not define ANY lat lons\n",
    "        bbox_data = get_minimum_bounding_rectangle()\n",
    "\n",
    "    return bbox_data\n",
    "\n",
    "def download_dem():\n",
    "    dem_cmd = [\n",
    "        \"{}/applications/dem.py\".format(ISCE_HOME), \"-a\",\n",
    "        \"stitch\", \"-b\", \"{} {} {} {}\".format(MINLAT_LO, MAXLAT_HI, MINLON_LO, MAXLON_HI),\n",
    "        \"-r\", \"-s\", \"1\", \"-f\", \"-c\", \"|\", \"tee\", \"dem.txt\"\n",
    "        #\"-n\", dem_user, \"-w\", dem_pass,\"-u\", dem_url\n",
    "    ]\n",
    "    run_cmd(dem_cmd)\n",
    "    \n",
    "def get_config_file(run_file, kward):\n",
    "    config_file = None\n",
    "    with open(run_file, 'r') as fp:\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            if kward in line:\n",
    "                config_file = line.split(' ')[-1]\n",
    "                break\n",
    "            line = fp.readline()\n",
    "    return config_file.strip()\n",
    "\n",
    "def get_config_val(config_file, p_name):\n",
    "    val = None\n",
    "    with open(config_file, 'r') as fp:\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            if p_name in line:\n",
    "                val = line.split(':')[-1].strip()\n",
    "                break\n",
    "            line = fp.readline()\n",
    "    return val\n",
    "\n",
    "def get_date_from_str(test_str):\n",
    "    import re\n",
    "    temp = re.findall(r'\\d+', test_str) \n",
    "    res = list(map(int, temp))\n",
    "    return str(res[0])\n",
    "\n",
    "def stackSlcDn_run2_5():\n",
    "    run1File = \"./run_files/run_01_unpack_topo_reference\"\n",
    "    run2File = \"./run_files/run_02_unpack_secondary_slc\"\n",
    "    runFile = \"./run_files/run_02.5_slc_noise_calibration\"\n",
    "\n",
    "    run_cmd(\"rm -f {}\".format(runFile).split(' '))\n",
    "    run_cmd(\"touch {}\".format(runFile).split(' '))\n",
    "    reference_dir = os.path.join(wd, \"reference\")\n",
    "    secondary_dir = os.path.join(wd, \"secondary\")\n",
    "    refConfig = get_config_file(run1File, \"reference\")\n",
    "    print(refConfig)\n",
    "    \n",
    "    ref_zip = get_config_val(refConfig, 'dirname')\n",
    "    print(ref_zip)\n",
    "    ref_swath = get_config_val(refConfig, 'swaths')\n",
    "    print(ref_swath)\n",
    "    \n",
    "    with open(runFile, 'w') as fw:\n",
    "        cmd = \"/opt/topsstack-hamsar/topsStack/read_calibration_slc.py -zip {} -ext {} -od {} -o -t noise -n '{}'\\n\".format(ref_zip, bbox, reference_dir, ref_swath)\n",
    "        fw.write(cmd)\n",
    "        with open(run2File, 'r') as fp:\n",
    "            line = fp.readline()\n",
    "            cmds = []\n",
    "            while line:\n",
    "                if 'secondary' in line:\n",
    "                    secConfig = line.split('-c')[-1].strip()\n",
    "                    sec_date = get_date_from_str(os.path.basename(secConfig))\n",
    "                    print(\"{} : {}\".format(sec_date, os.path.basename(secConfig)))\n",
    "                    sec_zip = get_config_val(secConfig, 'dirname')\n",
    "                    sec_swath = get_config_val(secConfig, 'swaths')\n",
    "                    odir=os.path.join(secondary_dir, sec_date )\n",
    "                    print(\"{} : {} :{}\".format(sec_zip, sec_swath, odir ))\n",
    "                    cmd = \"/opt/topsstack-hamsar/topsStack/read_calibration_slc.py -zip {} -ext {} -od {} -o -t noise -n '{}'\\n\".format(sec_zip, bbox, odir, sec_swath)\n",
    "                    fw.write(cmd)\n",
    "            \n",
    "                line = fp.readline()\n",
    "                                                                       \n",
    "                                                                       \n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import json\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from osgeo import ogr, osr\n",
    "\n",
    "def get_union_polygon_from_bbox(env):\n",
    "    coords = [\n",
    "        [ env[3], env[0] ],\n",
    "        [ env[3], env[1] ],\n",
    "        [ env[2], env[1] ],\n",
    "        [ env[2], env[0] ],\n",
    "    ]\n",
    "    return {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [ coords ]\n",
    "    }\n",
    "\n",
    "# copied from stitch_ifgs.get_union_polygon()\n",
    "def get_union_polygon(ds_files):\n",
    "    \"\"\"\n",
    "    Get GeoJSON polygon of union of IFGs.\n",
    "    :param ds_files: list of .dataset.json files, which have the 'location' key\n",
    "    :return: geojson of merged bbox\n",
    "    \"\"\"\n",
    "\n",
    "    geom_union = None\n",
    "    for ds_file in ds_files:\n",
    "        f = open(ds_file)\n",
    "        ds = json.load(f)\n",
    "        geom = ogr.CreateGeometryFromJson(json.dumps(ds['location'], indent=2, sort_keys=True))\n",
    "        if geom_union is None:\n",
    "            geom_union = geom\n",
    "        else:\n",
    "            geom_union = geom_union.Union(geom)\n",
    "    return json.loads(geom_union.ExportToJson()), geom_union.GetEnvelope()\n",
    "\n",
    "\n",
    "def get_dataset_met_json_files(cxt):\n",
    "    \"\"\"\n",
    "    returns 2 lists: file paths for dataset.json files and met.json files\n",
    "    :param cxt: json from _context.json\n",
    "    :return: list[str], list[str]\n",
    "    \"\"\"\n",
    "    pwd = os.getcwd()\n",
    "    localize_urls = cxt['localize_slcs']\n",
    "\n",
    "    met_files, ds_files = [], []\n",
    "    for localize_url in localize_urls:\n",
    "        local_path = localize_url['local_path']\n",
    "        slc_id = local_path.split('/')[0]\n",
    "        slc_path = os.path.join(pwd, slc_id, slc_id)\n",
    "\n",
    "        ds_files.append(slc_path + '.dataset.json')\n",
    "        met_files.append(slc_path + '.met.json')\n",
    "    return ds_files, met_files\n",
    "\n",
    "\n",
    "def get_scenes(cxt):\n",
    "    \"\"\"\n",
    "    gets all SLC scenes for the stack\n",
    "    :param cxt: contents for _context.json\n",
    "    :return: list of scenes\n",
    "    \"\"\"\n",
    "    localize_urls = cxt['localize_slcs']\n",
    "    all_scenes = set()\n",
    "    for localize_url in localize_urls:\n",
    "        local_path = localize_url['local_path']\n",
    "        slc_id = local_path.split('/')[0]\n",
    "        all_scenes.add(slc_id)\n",
    "    return sorted(list(all_scenes))\n",
    "\n",
    "\n",
    "def get_min_max_timestamps(scenes_ls):\n",
    "    \"\"\"\n",
    "    returns the min timestamp and max timestamp of the stack\n",
    "    :param scenes_ls: list[str] all slc scenes in stack\n",
    "    :return: (str, str) 2 timestamp strings, ex. 20190518T161611\n",
    "    \"\"\"\n",
    "    timestamps = set()\n",
    "\n",
    "    regex_pattern = r'(\\d{8}T\\d{6}).(\\d{8}T\\d{6})'\n",
    "    for scene in scenes_ls:\n",
    "        matches = re.search(regex_pattern, scene)\n",
    "        if not matches:\n",
    "            raise Exception(\"regex %s was unable to match with SLC id %s\" % (regex_pattern, scene))\n",
    "\n",
    "        slc_timestamps = (matches.group(1), matches.group(2))\n",
    "        timestamps = timestamps.union(slc_timestamps)\n",
    "\n",
    "    min_timestamp = min(timestamps)\n",
    "    max_timestamp = max(timestamps)\n",
    "    return min_timestamp.replace('T', ''), max_timestamp.replace('T', '')\n",
    "\n",
    "def get_min_max_times(scenes_ls):\n",
    "    \"\"\"\n",
    "    returns the min timestamp and max timestamp of the stack\n",
    "    :param scenes_ls: list[str] all slc scenes in stack\n",
    "    :return: (str, str) 2 timestamp strings, ex. 20190518T161611\n",
    "    \"\"\"\n",
    "    timestamps = set()\n",
    "\n",
    "    regex_pattern = r'(\\d{8}T\\d{6}).(\\d{8}T\\d{6})'\n",
    "    for scene in scenes_ls:\n",
    "        matches = re.search(regex_pattern, scene)\n",
    "        if not matches:\n",
    "            raise Exception(\"regex %s was unable to match with SLC id %s\" % (regex_pattern, scene))\n",
    "\n",
    "        slc_timestamps = (matches.group(1), matches.group(2))\n",
    "        timestamps = timestamps.union(slc_timestamps)\n",
    "\n",
    "    min_timestamp = min(timestamps)\n",
    "    max_timestamp = max(timestamps)\n",
    "    return min_timestamp, max_timestamp\n",
    "\n",
    "\n",
    "def create_list_from_keys_json_file(json_files, *args):\n",
    "    \"\"\"\n",
    "    gets all key values in each .json file and returns a sorted array of values\n",
    "    :param json_files: list[str]\n",
    "    :return: list[]\n",
    "    \"\"\"\n",
    "    values = set()\n",
    "    for json_file in json_files:\n",
    "        if os.path.isfile(json_file): \n",
    "            f = open(json_file)\n",
    "            data = json.load(f)\n",
    "            for arg in args:\n",
    "                value = data[arg]\n",
    "                values.add(value)\n",
    "    return sorted(list(values))\n",
    "\n",
    "\n",
    "def camelcase_to_underscore(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "\n",
    "def get_key_and_convert_to_underscore(json_file_paths, key):\n",
    "    \"\"\"\n",
    "    read through all the json files in file paths, get the first occurrence of key and convert it to underscore\n",
    "    :param json_file_paths: list[str]\n",
    "    :param key: str\n",
    "    :return: key and value\n",
    "    \"\"\"\n",
    "    for json_file in json_file_paths:\n",
    "        if os.path.isfile(json_file): \n",
    "            f = open(json_file)\n",
    "            data = json.load(f)\n",
    "            if key in data.keys():\n",
    "                underscore_key = camelcase_to_underscore(key)\n",
    "                return underscore_key, data[key]\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def generate_dataset_json_data(ctx, version):\n",
    "    \"\"\"\n",
    "    :param cxt: _context.json file\n",
    "    :param dataset_json_files: list[str] all file paths of SLC's .dataset.json files\n",
    "    :param version: str: version, ex. v1.0\n",
    "    :return: dict\n",
    "    \"\"\"\n",
    "    dataset_json_data = dict()\n",
    "    dataset_json_data['version'] = version\n",
    "\n",
    "    \n",
    "    try:      \n",
    "        dataset_json_data['starttime'], dataset_json_data['endtime']  = get_min_max_times(ctx[\"localize_slcs\"])      \n",
    "    except Exception as err:\n",
    "        print(str(err))\n",
    "    try:\n",
    "        dataset_json_data['location'] = get_union_polygon_from_bbox(bbox.split(' '))\n",
    "    except Exception as err:\n",
    "        print(str(err))\n",
    "    return dataset_json_data\n",
    "\n",
    "\n",
    "def generate_met_json_data(ctx, bbox, version):\n",
    "    \"\"\"\n",
    "    :param cxt: _context.json file\n",
    "    :param met_json_file_paths: list[str] all file paths of SLC's .met.json files\n",
    "    :param dataset_json_files: list[str] all file paths of SLC's .dataset.json files\n",
    "    :param version: str: version, ex. v1.0\n",
    "    :return: dict\n",
    "    \"\"\"\n",
    "    met_json_data = {\n",
    "        'processing_start': PROCESSING_START,\n",
    "        'processing_stop': datetime.now().strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "        'version': version\n",
    "    }\n",
    "\n",
    "    # generating bbox\n",
    "    geojson = get_union_polygon_from_bbox(bbox)\n",
    "    coordinates = geojson['coordinates'][0]\n",
    "    for coordinate in coordinates:\n",
    "        coordinate[0], coordinate[1] = coordinate[1], coordinate[0]\n",
    "    \n",
    "    met_json_data['bbox'] = bbox\n",
    "    \n",
    "    # list of SLC scenes\n",
    "   \n",
    "    met_json_data['scenes'] = ctx[\"localize_slcs\"]\n",
    "    met_json_data['scene_count'] = len(ctx[\"localize_slcs\"])\n",
    "\n",
    "    # getting timestamps\n",
    "\n",
    "    met_json_data['sensing_start'], met_json_data['sensing_stop']  = get_min_max_times(ctx[\"localize_slcs\"])\n",
    "\n",
    "    # additional information\n",
    "    met_json_data['dataset_type'] = 'stack'\n",
    "\n",
    "    return met_json_data\n",
    "\n",
    "\n",
    "def read_context():\n",
    "    with open('_context.json', 'r') as f:\n",
    "        cxt = json.load(f)\n",
    "        return cxt\n",
    "\n",
    "\n",
    "def create_dataset(bbox):\n",
    "    VERSION = 'v1.0'\n",
    "    DATASET_NAMING_TEMPLATE = 'coregistered_slcs-{min_timestamp}-{max_timestamp}'\n",
    "    PWD = os.getcwd()\n",
    "\n",
    "    # creating list of all SLC .dataset.json and .met.json files\n",
    "    ctx = read_context()\n",
    "    #dataset_json_files, met_json_files = get_dataset_met_json_files(context_json)\n",
    "\n",
    "    # getting list of SLC scenes and extracting min max timestamp\n",
    "    slc_scenes = ctx['localize_slcs']\n",
    "    min_timestamp, max_timestamp = get_min_max_timestamps(slc_scenes)\n",
    "\n",
    "    # creatin dataset directory\n",
    "    dataset_name = DATASET_NAMING_TEMPLATE.format(min_timestamp=min_timestamp, max_timestamp=max_timestamp)\n",
    "    if not os.path.exists(dataset_name):\n",
    "        os.mkdir(dataset_name)\n",
    "\n",
    "    # move merged/ master/ slaves/ directory to dataset directory\n",
    "    move_directories = ['merged', 'reference', 'secondarys']\n",
    "    for directory in move_directories:\n",
    "        shutil.move(directory, dataset_name)\n",
    "\n",
    "    # move _stdout.txt log file to dataset\n",
    "    shutil.copyfile('_stdout.txt', os.path.join(dataset_name, '_stdout.txt'))\n",
    "\n",
    "    # generate .dataset.json data\n",
    "    dataset_json_data = {}\n",
    "    try:\n",
    "        dataset_json_data = generate_dataset_json_data(ctx, VERSION)\n",
    "    except Exception as err:\n",
    "        print(str(err))\n",
    "    dataset_json_data['label'] = dataset_name\n",
    "    print(json.dumps(dataset_json_data, indent=2))\n",
    "\n",
    "    # generate .met.json data\n",
    "    met_json_data = generate_met_json_data(ctx, bbox, VERSION)\n",
    "    print(json.dumps(met_json_data, indent=2))\n",
    "\n",
    "    # writing .dataset.json to file\n",
    "    dataset_json_filename = os.path.join(PWD, dataset_name, dataset_name + '.dataset.json')\n",
    "    with open(dataset_json_filename, 'w') as f:\n",
    "        json.dump(dataset_json_data, f, indent=2)\n",
    "\n",
    "    # writing .met.json to file\n",
    "    met_json_filename = os.path.join(PWD, dataset_name, dataset_name + '.met.json')\n",
    "    with open(met_json_filename, 'w') as f:\n",
    "        json.dump(met_json_data, f, indent=2)\n",
    "        \n",
    "        \n",
    "        \n",
    "def download_slcs(path):\n",
    "    \n",
    "    for slc in ctx[\"localize_slcs\"]:\n",
    "        download_slc(slc, path)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "MINLAT, MAXLAT, MINLON, MAXLON, MINLAT_LO, MAXLAT_HI, MINLON_LO, MAXLON_HI =get_bbox(ctx)\n",
    "print(\"{} {} {} {} {} {} {} {}\".format(MINLAT, MAXLAT, MINLON, MAXLON, MINLAT_LO, MAXLAT_HI, MINLON_LO, MAXLON_HI))\n",
    "\n",
    "bbox = \"{} {} {} {}\".format(MINLAT, MAXLAT, MINLON, MAXLON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download Sentinel-1 data SLC**\n",
    "\n",
    "The SLCs identified in the parameters cell are downloaded. Note that these files are significant in size (around 5GB each) which will collectively require a significant amount of time to download (expect around 2 minutes per SLC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = \"zip\"\n",
    "download_slcs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Download Orbit Files based on SLC***\n",
    "\n",
    "The orbit files identified based on the set of SLCs downloaded in the previous step. They are placed in the orbits subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_orbit_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download DEM data and generate DEM file**\n",
    "Similarly to the orbit files, the DEM information to retain is computed based on the bounding box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fix paths in dem file**\n",
    "\n",
    "This step invokes the isce2 application fixImageXML.py in order to fill in the complete path to the files referenced in the dem file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(\"cwd : {}\".format(cwd))\n",
    "if os.path.exists(\"dem.txt\"):\n",
    "    cmd = [\"awk\", \"'/wgs84/ {print $NF;exit}'\", \"dem.txt\"]\n",
    "    WGS84 = run_cmd_output(cmd).decode(\"utf-8\").strip()\n",
    "    wgs84_file = os.path.join(cwd, WGS84)\n",
    "    print(\"WGS84 : a{}b\".format(wgs84_file))\n",
    "    if os.path.exists(wgs84_file):\n",
    "        print(\"Found wgs84 file: {}\".format(wgs84_file))\n",
    "        fix_cmd = [\"{}/applications/fixImageXml.py\".format(ISCE_HOME), \"--full\", \"-i\", \"{}\".format(wgs84_file) ]\n",
    "        run_cmd(fix_cmd) \n",
    "    else:\n",
    "        print(\"NO WGS84 FILE FOUND : {}\".format(wgs84_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUX_CAL file download ####\n",
    "\n",
    "The following calibration auxliary (AUX_CAL) file is used for **antenna pattern correction** to compensate the range phase offset of SAFE products with **IPF verison 002.36** (mainly for images acquired before March 2015). If all your SAFE products are from another IPF version, then no AUX files are needed. Check [ESA document](https://earth.esa.int/documents/247904/1653440/Sentinel-1-IPF_EAP_Phase_correction) for details. \n",
    "\n",
    "Run the command below to download the AUX_CAL file once and store it somewhere (_i.e._ ~/aux/aux_cal) so that you can use it all the time, for `stackSentinel.py -a` or `auxiliary data directory` in `topsApp.py`.\n",
    "\n",
    "```\n",
    "wget https://qc.sentinel1.eo.esa.int/product/S1A/AUX_CAL/20140908T000000/S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ\n",
    "tar zxvf S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ\n",
    "rm S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ./AuxDir\n",
    "wget https://qc.sentinel1.eo.esa.int/product/S1A/AUX_CAL/20140908T000000/S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ\n",
    "tar zxvf S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ --directory ./AuxDir\n",
    "rm S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The scripts provides support for Sentinel-1 TOPS stack processing. Currently supported workflows include a coregistered stack of SLC, interferograms, offsets, and coherence. \n",
    "\n",
    "`stackSentinel.py` generates all configuration and run files required to be executed on a stack of Sentinel-1 TOPS data. When stackSentinel.py is executed for a given workflow (-W option) a **configs** and **run_files** folder is generated. No processing is performed at this stage. Within the run_files folder different run\\_#\\_description files are contained which are to be executed as shell scripts in the run number order. Each of these run scripts call specific configure files contained in the “configs” folder which call ISCE in a modular fashion. The configure and run files will change depending on the selected workflow. To make run_# files executable, change the file permission accordingly (e.g., `chmod +x run_01_unpack_slc`).\n",
    "\n",
    "```bash\n",
    "stackSentinel.py -H     #To see workflow examples,\n",
    "stackSentinel.py -h     #To get an overview of all the configurable parameters\n",
    "```\n",
    "\n",
    "Required parameters of stackSentinel.py include:\n",
    "\n",
    "```cfg\n",
    "-s SLC_DIRNAME          #A folder with downloaded Sentinel-1 SLC’s. \n",
    "-o ORBIT_DIRNAME        #A folder containing the Sentinel-1 orbits. Missing orbit files will be downloaded automatically\n",
    "-a AUX_DIRNAME          #A folder containing the Sentinel-1 Auxiliary files\n",
    "-d DEM_FILENAME         #A DEM (Digital Elevation Model) referenced to wgs84\n",
    "```\n",
    "\n",
    "In the following, different workflow examples are provided. Note that stackSentinel.py only generates the run and configure files. To perform the actual processing, the user will need to execute each run file in their numbered order.\n",
    "\n",
    "In all workflows, coregistration (-C option) can be done using only geometry (set option = geometry) or with geometry plus refined azimuth offsets through NESD (set option = NESD) approach, the latter being the default. For the NESD coregistrstion the user can control the ESD coherence threshold (-e option) and the number of overlap interferograms (-O) to be used in NESD estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_date=get_master_date(ctx)\n",
    "print(\"master_date : {}\".format(master_date))\n",
    "\n",
    "if master_date:\n",
    "    print(\"MASTER_DATE exists:\".format(master_date) )   \n",
    "    cmd = [\n",
    "        \"/opt/isce2/src/isce2/contrib/stack/topsStack/stackSentinel.py\",  \"-s\", \"zip/\", \"-d\", \"{}\".format(wgs84_file), \"-a\", \"AuxDir/\", \"-m\", \"{}\".format(master_date), \"-o\", \"orbits\", \n",
    "        \"-b\", \"\\\"{} {} {} {}\\\"\".format(MINLAT, MAXLAT, MINLON, MAXLON), \n",
    "        \"-W\", \"slc\", \"-C\", \"geometry\"\n",
    "    ]\n",
    "else:\n",
    "    print(\"MASTER_DATE DOES NOT EXIST\")          \n",
    "    cmd = [\n",
    "        \"/opt/isce2/src/isce2/contrib/stack/topsStack/stackSentinel.py\",  \"-s\", \"zip/\", \"-d\", \"{}\".format(wgs84_file), \"-a\", \"AuxDir/\", \"-o\", \"orbits\", \n",
    "        \"-b\", \"\\\"{} {} {} {}\\\"\".format(MINLAT, MAXLAT, MINLON, MAXLON), \n",
    "        \"-W\", \"slc\", \"-C\", \"geometry\"\n",
    "    ]\n",
    "run_cmd(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stackSlcDn_run2_5**\n",
    "\n",
    "An intermediate run file must be manually added between run files 2 and 3 in order to perform radiometric and thermal noise cancellation. This step generates this run file, executing against each SLC file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackSlcDn_run2_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**run_01_unpack_slc_topo_reference:**\n",
    "Unpacks the reference SLC files using ISCE readers, also unpackaging the antenna elevation pattern correction file if necessary. This run file also produces the reference geometry files that are consumed downstream.\n",
    "\n",
    "Includes commands to unpack Sentinel-1 TOPS SLCs using ISCE readers. For older SLCs which need antenna elevation pattern correction, the file is extracted and written to disk. For newer version of SLCs which don’t need the elevation antenna pattern correction, only a gdal virtual “vrt” file (and isce xml file) is generated. The “.vrt” file points to the Sentinel SLC file and reads them whenever required during the processing. If a user wants to write the “.vrt” SLC file to disk, it can be done easily using gdal_translate (e.g. gdal_translate –of ENVI File.vrt File.slc). \n",
    "The “run_01_unpack_slc_topo_reference” also includes a command that refers to the config file of the stack reference, which includes configuration for running topo for the stack reference. Note that in the pair-wise processing strategy one should run topo (mapping from range-Doppler to geo coordinate) for all pairs. However, with stackSentinel, topo needs to be run only one time for the reference in the stack. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"## STEP 1 ##\"\n",
    "source /opt/isce2/isce_env.sh\n",
    "export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:${PATH}\n",
    "export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:${PYTHONPATH}\n",
    "source /opt/conda/etc/profile.d/conda.sh\n",
    "conda activate base\n",
    "\n",
    "\n",
    "start=`date +%s`\n",
    "echo \"sh run_files/run_01_unpack_topo_reference\"\n",
    "sh run_files/run_01_unpack_topo_reference\n",
    "end=`date +%s`\n",
    "runtime1=$((end-start))\n",
    "echo \"STEP 1 RUN TIME : $runtime1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**run_02_unpack_secondary_slc:**\n",
    "\n",
    "Unpack Secondary SLCs\n",
    "In a manner similar to the SLCs in Run File 01 above, this run file unpacks the secondary SLCs from each of the input SLC zip files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"## STEP 2 ##\"\n",
    "source /opt/isce2/isce_env.sh\n",
    "export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:${PATH}\n",
    "export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:${PYTHONPATH}\n",
    "source /opt/conda/etc/profile.d/conda.sh\n",
    "\n",
    "start=`date +%s`\n",
    "Num=`cat run_files/run_02_unpack_secondary_slc | wc | awk '{print $1}'`\n",
    "echo $Num\n",
    "echo \"cat run_files/run_02_unpack_secondary_slc | parallel -j2 --eta --load 50%\"\n",
    "cat run_files/run_02_unpack_secondary_slc | parallel -j2 --eta --load 50%\n",
    "end=`date +%s`\n",
    "\n",
    "runtime2=$((end-start))\n",
    "echo STEP 2 RUN TIME : $runtime2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run_02.5_slc_noise_calibration ###\n",
    "This run file, which was manually introduced in step 19, runs radiometric and thermal noise calibration using vh-pol data against the SLCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"## STEP 2.5 ##\"\n",
    "source /opt/isce2/isce_env.sh\n",
    "export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:~/topsstack-hamsar/topsStack/:${PATH}\n",
    "export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:~/topsstack-hamsar/topsStack/:${PYTHONPATH}\n",
    "source /opt/conda/etc/profile.d/conda.sh\n",
    "conda activate base\n",
    "\n",
    "start=`date +%s`\n",
    "echo \"cat run_files/run_02.5_slc_noise_calibration | parallel -j2 --eta --load 50%\"\n",
    "cat run_files/run_02.5_slc_noise_calibration | parallel -j2 --eta --load 50%\n",
    "end=`date +%s`\n",
    "\n",
    "runtime2x5=$((end-start))\n",
    "echo STEP 2.5 RUN TIME : $runtime2x5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**run_03_average_baseline:**\n",
    "\n",
    "Computes average baseline for the stack. These baselines are not used for processing anywhere. They are only an approximation and can be used for plotting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"## STEP 3 ##\"\n",
    "source /opt/isce2/isce_env.sh\n",
    "export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:${PATH}\n",
    "export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:${PYTHONPATH}\n",
    "source /opt/conda/etc/profile.d/conda.sh\n",
    "conda activate base\n",
    "\n",
    "start=`date +%s`\n",
    "echo \"cat run_files/run_03_average_baseline | parallel -j2 --eta --load 50%\"\n",
    "cat run_files/run_03_average_baseline | parallel -j2 --eta --load 50%\n",
    "end=`date +%s`\n",
    "runtime3=$((end-start))\n",
    "echo STEP 3 RUN TIME : $runtime3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 4 : run_04_fullBurst_geo2rdr:**\n",
    "\n",
    "Full Burst geo2rdr\n",
    "This run file estimates geometrical offsets between secondary burst overlaps and the stack reference burst overlaps. The secondaries are then resampled to the stack reference burst overlaps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"## STEP 4 ##\"\n",
    "source /opt/isce2/isce_env.sh\n",
    "export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:${PATH}\n",
    "export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:${PYTHONPATH}\n",
    "source /opt/conda/etc/profile.d/conda.sh\n",
    "conda activate base\n",
    "\n",
    "start=`date +%s`\n",
    "\n",
    "echo \"cat run_files/run_04_fullBurst_geo2rdr  | parallel -j2 --eta --load 50%\"\n",
    "cat run_files/run_04_fullBurst_geo2rdr  | parallel -j2 --eta --load 50%\n",
    "end=`date +%s`\n",
    "runtime4=$((end-start))\n",
    "echo STEP 4 RUN TIME : $runtime4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 5 : run_05_fullBurst_resample:**\n",
    "\n",
    "Using orbit and DEM data, this run file computes geometrical offsets among all secondary SLCs and the stack reference. These offsets, with the misregistration time series are used for precise coregistration of each burst SLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"## STEP 5 ##\"\n",
    "source /opt/isce2/isce_env.sh\n",
    "export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:${PATH}\n",
    "export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:${PYTHONPATH}\n",
    "source /opt/conda/etc/profile.d/conda.sh\n",
    "conda activate base\n",
    "\n",
    "start=`date +%s`\n",
    "echo \"cat run_files/run_05_fullBurst_resample  | parallel -j2 --eta --load 50%\"\n",
    "cat run_files/run_05_fullBurst_resample  | parallel -j2 --eta --load 50%\n",
    "end=`date +%s`\n",
    "runtime5=$((end-start))\n",
    "echo STEP 5 RUN TIME : $runtime5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 6 : run_06_timeseries_misreg:**\n",
    "\n",
    "This run file extracts the valid region between burst SLCs at the overlap area of the bursts. This region changes slightly for different acquisitions and must be retained for merging the bursts to eliminate invalid data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"## STEP 6 ##\"\n",
    "source /opt/isce2/isce_env.sh\n",
    "export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:${PATH}\n",
    "export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:${PYTHONPATH}\n",
    "source /opt/conda/etc/profile.d/conda.sh\n",
    "conda activate base\n",
    "\n",
    "start=`date +%s`\n",
    "echo \"sh run_files/run_06_extract_stack_valid_region\"\n",
    "sh run_files/run_06_extract_stack_valid_region\n",
    "end=`date +%s`\n",
    "runtime6=$((end-start))\n",
    "echo STEP 6 RUN TIME : $runtime6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**run_07_geo2rdr_resample: Merge Reference Secondary SLCs**\n",
    "\n",
    "This run file merges all bursts for the reference and coregistered SLCs. Geometry files are also merged.\n",
    "Using orbit and DEM, geometrical offsets among all secondary SLCs and the stack reference is computed. The geometrical offsets, together with the misregistration time-series (from previous step) are used for precise coregistration of each burst SLC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"## STEP 7 ##\"\n",
    "source /opt/isce2/isce_env.sh\n",
    "export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:${PATH}\n",
    "export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:${PYTHONPATH}\n",
    "source /opt/conda/etc/profile.d/conda.sh\n",
    "conda activate base\n",
    "\n",
    "start=`date +%s`\n",
    "\n",
    "FILE=run_files/run_07_merge\n",
    "if test -f \"$FILE\"; then\n",
    "    echo \"cat run_files/run_07_merge  | parallel -j2 --eta --load 50%\"\n",
    "    cat run_files/run_07_merge | parallel -j2 --eta --load 50%\n",
    "else\n",
    "    echo \"cat run_files/run_07_merge_reference_secondary_slc  | parallel -j2 --eta --load 50%\"\n",
    "    cat run_files/run_07_merge_reference_secondary_slc | parallel -j2 --eta --load 50%\n",
    "fi\n",
    "\n",
    "end=`date +%s`\n",
    "runtime7=$((end-start))\n",
    "echo STEP 7 RUN TIME : $runtime7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Dataset**\n",
    "The final step ‘packages’ up the results into a single sub-directory named as coregistered_slcs-<start_date>-<end_date> and generates json files describing the dataset (e.g. the polygon  coordinates of the bounding box) and associated metadata (e.g. included SLCs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(bbox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_stop_time = get_current_time()\n",
    "print(\"PGE run stop time : {}\".format(run_stop_time))\n",
    "total_run_time = run_stop_time - run_start_time\n",
    "print(\"Total Run Time = {}\".format(total_run_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
